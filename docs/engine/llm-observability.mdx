import { Conversation, User, Assistant } from "../../src/components/convengine";

# ğŸ” LLM Observability

ConvEngine treats **LLM calls as first-class, auditable events** â€” not hidden side effects.
Every prompt, context, configuration, and response is **fully logged** for traceability, replay, and compliance.

---

## ğŸ§  Why LLM Observability Matters

LLMs are:
- Non-deterministic
- External dependencies
- Hard to debug after the fact

ConvEngine solves this by **persisting every LLM interaction**.

> If it influenced the user â†’ it must be observable.

---

## ğŸ—ƒï¸ The `ce_llm_call_log` Table

Each row represents **one logical LLM invocation**.

Key fields captured:

- `conversation_id` â€“ which conversation triggered the call
- `intent_code`, `state_code` â€“ engine context at call time
- `provider`, `model`, `temperature` â€“ full config snapshot
- `prompt_text` â€“ final system + instruction prompt
- `user_context` â€“ merged `context_json` + user input
- `response_text` â€“ raw LLM output
- `success`, `error_message` â€“ execution outcome

This enables:
- ğŸ” Replay
- ğŸ Debugging
- ğŸ“Š Analytics
- ğŸ›¡ï¸ Compliance audits

---

## ğŸ”„ When Does the Engine Call the LLM?

LLMs are invoked **only** in these cases:

- DERIVED text responses
- JSON extraction using `ce_output_schema`
- Follow-up question generation
- UI schema generation

> LLMs **never** change intent or state directly.

---

## ğŸ’¬ Example: Observed LLM Call

<Conversation
  engineStatus="RUNNING"
  engineDetails={
    <>
      <div className="ce-engine-item success">intent = ACTION</div>
      <div className="ce-engine-item warn">schema incomplete</div>
      <div className="ce-engine-item warn">invoking LLM (EXTRACT)</div>
    </>
  }
>
  <User>move from A</User>

  <Assistant>
    Where do you want to move the service?
  </Assistant>
</Conversation>

### What Gets Logged

- **Prompt**  
  â€œExtract structured data strictly matching the provided JSON schemaâ€¦â€

- **User Context**  
  `{ "from": { "subAccount": "A" } }`

- **Response**  
  Partial JSON (missing `to.subAccount`)

- **Outcome**  
  `success = true`

---

## ğŸ§ª Debugging & Replay

Because prompts and context are stored:

- You can re-run the same prompt later
- Compare responses across models
- Detect prompt regressions
- Audit production behavior

This is critical for **enterprise trust**.

---

## ğŸš« What ConvEngine Explicitly Avoids

- âŒ No silent LLM calls
- âŒ No hidden prompt mutations
- âŒ No state mutation by LLM output

Every call is:
âœ” Explicit  
âœ” Logged  
âœ” Reproducible  

---

## ğŸ Takeaway

ConvEngine doesnâ€™t just *use* LLMs â€”  
it **observes, governs, and audits** them.

This is what makes the engine:
- Predictable
- Debuggable
- Production-ready
